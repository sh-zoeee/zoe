{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import ot\n",
    "\n",
    "from scripts import w_pq_batch as w_pq\n",
    "from scripts import trees, pq_gram, func\n",
    "\n",
    "from pqgrams.PQGram import Profile\n",
    "import pyconll\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EWT,Atis間のWasserstein距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors_path = \"data/train_tensors_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "train_labels_path = \"data/train_labels_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "train_indexes_path = \"data/train_indexes_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "\n",
    "valid_tensors_path = \"data/valid_tensors_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "valid_labels_path = \"data/valid_labels_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "valid_indexes_path = \"data/valid_indexes_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "\n",
    "test_tensors_path = \"data/test_tensors_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "test_labels_path = \"data/test_labels_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "test_indexes_path = \"data/test_indexes_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "\n",
    "model_path = \"models/model_en_corpora_EWT_Atis_unlabel_50.pth\"\n",
    "\n",
    "CoNLLU_EWT_PATH = \"corpora/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_EWT_PATH)\n",
    "EWT_tree_count = len(CoNLLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = torch.load(train_tensors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-2.0425, -2.2598, -1.7006, -2.2871, -2.0425,  0.3741, -1.7006, -1.7006],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_function = w_pq.WeightedPqgramDistance(train_tensors[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTマトリックス: 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "a = [0.3, 0.7]  # ソース分布（サイズ2）\n",
    "b = [0.4, 0.4, 0.2]  # ターゲット分布（サイズ3）\n",
    "M = [[0.0, 1.0, 2.0],  # 2x3 のコスト行列\n",
    "     [1.0, 0.5, 0.5]]\n",
    "\n",
    "T = ot.emd2(a, b, M)\n",
    "print(\"OTマトリックス:\", T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoNLLU_GPT_PATH = \"corpora/English-chatGPT.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[convert tensor]: 100%|██████████| 30015/30015 [00:00<00:00, 41490.56it/s]\n"
     ]
    }
   ],
   "source": [
    "CoNLLU += pyconll.load_from_file(CoNLLU_GPT_PATH)\n",
    "\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index, desc=\"[convert tensor]\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_distance_weighted(data1, data2, weights):\n",
    "    # 重みを適用\n",
    "    weights = func.softplus(weights)\n",
    "    weighted_data1 = [t*weights for t in data1]  # 各サンプルに重みを適用\n",
    "    weighted_data2 = [t*weights for t in data2]\n",
    "\n",
    "    distances = []\n",
    "    for dim in range(weighted_data1[0].size(0)):  # 8次元でループ\n",
    "        # dim次元の要素を全て取得して連結\n",
    "        x_dim = torch.cat([t[dim].unsqueeze(0) for t in weighted_data1])\n",
    "        y_dim = torch.cat([t[dim].unsqueeze(0) for t in weighted_data2])\n",
    "\n",
    "        # 次元ごとに要素をソート\n",
    "        x_sorted = torch.sort(x_dim)[0]\n",
    "        y_sorted = torch.sort(y_dim)[0]\n",
    "\n",
    "        # 累積分布関数 (CDF) を計算\n",
    "        cdf_x = torch.cumsum(torch.ones_like(x_sorted) / len(x_sorted), dim=0)\n",
    "        cdf_y = torch.cumsum(torch.ones_like(y_sorted) / len(y_sorted), dim=0)\n",
    "\n",
    "        # 各次元のWasserstein距離を計算\n",
    "        distance = torch.mean(torch.abs(cdf_x - cdf_y))\n",
    "        distances.append(distance)\n",
    "\n",
    "    print(distances)\n",
    "    # 各次元の距離の平均を返す\n",
    "    return torch.mean(torch.tensor(distances)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightedPqgramDistance()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_function = w_pq.WeightedPqgramDistance(tensors[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = distance_function.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13394\n"
     ]
    }
   ],
   "source": [
    "print(min(EWT_tree_count, len(tensors)-EWT_tree_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_EWT = tensors[:EWT_tree_count]\n",
    "tensors_GPT = tensors[EWT_tree_count:]\n",
    "sample_size = min(EWT_tree_count, len(tensors)-EWT_tree_count)\n",
    "if EWT_tree_count<sample_size:\n",
    "    tensors_GPT = random.sample(tensors_GPT, k=sample_size)\n",
    "else :\n",
    "    tensors_EWT = random.sample(tensors_EWT, k=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensors_EWT = torch.zeros((EWT_tree_count,8))\n",
    "for i, tensor in enumerate(tensors_EWT):\n",
    "    Tensors_EWT[i] = tensor\n",
    "\n",
    "Tensors_GPT = torch.zeros((len(tensors_GPT),8))\n",
    "for i, tensor in enumerate(tensors_GPT):\n",
    "    Tensors_GPT[i] = tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix_chunked(tensors: np.ndarray, weights: np.ndarray, chunk_size: int):\n",
    "    \"\"\"\n",
    "    データをチャンクに分割して距離行列を計算する関数。\n",
    "    tensors: 入力データ [N, dim] の配列\n",
    "    weights: 重み [dim] の配列\n",
    "    chunk_size: 一度に処理するデータのチャンクサイズ\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples = tensors.shape[0]\n",
    "    dist_mat = np.zeros((num_samples, num_samples))  # 距離行列の初期化\n",
    "    \n",
    "    # チャンクごとに計算\n",
    "    for i in tqdm(range(0, num_samples, chunk_size)):\n",
    "        end_i = min(i + chunk_size, num_samples)\n",
    "        tensor_chunk_i = tensors[i:end_i, np.newaxis]  # [chunk_size, 1, dim]\n",
    "\n",
    "        for j in range(0, num_samples, chunk_size):\n",
    "            end_j = min(j + chunk_size, num_samples)\n",
    "            tensor_chunk_j = tensors[j:end_j, np.newaxis]  # [1, chunk_size, dim]\n",
    "            \n",
    "            # 差の計算\n",
    "            diff = np.abs(tensor_chunk_i - tensor_chunk_j)  # [chunk_size, chunk_size, dim]\n",
    "            aw = np.log1p(np.exp(weights))  # weightsのSoftplus関数を近似\n",
    "            aw = aw[np.newaxis, np.newaxis, :]  # [1, 1, dim]\n",
    "            weighted_diff = diff * aw  # アダマール積\n",
    "            dist_chunk = weighted_diff.sum(axis=2)  # 距離の計算\n",
    "            \n",
    "            # 距離行列に結果を格納\n",
    "            dist_mat[i:end_i, j:end_j] = dist_chunk\n",
    "\n",
    "    return dist_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16621\n"
     ]
    }
   ],
   "source": [
    "array_EWT = Tensors_EWT.numpy()\n",
    "array_GPT = Tensors_GPT.numpy()\n",
    "\n",
    "weights_np = weights.detach().numpy()\n",
    "\n",
    "array_size = len(array_EWT) + len(array_GPT)\n",
    "\n",
    "array_all = np.zeros((array_size, 8))\n",
    "\n",
    "print(len(array_EWT))\n",
    "\n",
    "for i in range(len(array_EWT)):\n",
    "    array_all[i] = array_EWT[i]\n",
    "\n",
    "for i in range(len(array_GPT)):\n",
    "    array_all[i+len(array_GPT)] = array_GPT[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix_chunked(tensors: torch.Tensor, weights: torch.Tensor, chunk_size: int):\n",
    "    \"\"\"\n",
    "    データをチャンクに分割して距離行列を計算する関数。\n",
    "    tensors: 入力データ [N, dim] のテンソル\n",
    "    weights: 重み [dim] のテンソル\n",
    "    chunk_size: 一度に処理するデータのチャンクサイズ\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    num_samples = tensors.shape[0]\n",
    "    dist_mat = torch.zeros((num_samples, num_samples), device=device)  # 距離行列の初期化\n",
    "    \n",
    "    # チャンクごとに計算\n",
    "    for i in range(0, num_samples, chunk_size):\n",
    "        end_i = min(i + chunk_size, num_samples)\n",
    "        tensor_chunk_i = tensors[i:end_i].unsqueeze(1)  # [chunk_size, 1, dim]\n",
    "\n",
    "        for j in range(0, num_samples, chunk_size):\n",
    "            end_j = min(j + chunk_size, num_samples)\n",
    "            tensor_chunk_j = tensors[j:end_j].unsqueeze(0)  # [1, chunk_size, dim]\n",
    "            \n",
    "            # 差の計算\n",
    "            diff = torch.abs(tensor_chunk_i - tensor_chunk_j).to(device)  # [chunk_size, chunk_size, dim]\n",
    "            aw = func.softplus(weights).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, dim]\n",
    "            weighted_diff = diff * aw  # アダマール積\n",
    "            dist_chunk = weighted_diff.sum(dim=2)  # 距離の計算\n",
    "            \n",
    "            # 距離行列に結果を格納\n",
    "            dist_mat[i:end_i, j:end_j] = dist_chunk\n",
    "        del dist_chunk, end_j, aw, weighted_diff\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    del tensors, end_i\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 3 has a total capacity of 23.65 GiB of which 3.19 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m distance_matrix\u001b[38;5;241m=\u001b[39m\u001b[43mdistance_matrix_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_all\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mdistance_matrix_chunked\u001b[0;34m(tensors, weights, chunk_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m diff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(tensor_chunk_i \u001b[38;5;241m-\u001b[39m tensor_chunk_j)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# [chunk_size, chunk_size, dim]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m aw \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39msoftplus(weights)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [1, 1, dim]\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m weighted_diff \u001b[38;5;241m=\u001b[39m \u001b[43mdiff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maw\u001b[49m  \u001b[38;5;66;03m# アダマール積\u001b[39;00m\n\u001b[1;32m     26\u001b[0m dist_chunk \u001b[38;5;241m=\u001b[39m weighted_diff\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 距離の計算\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 距離行列に結果を格納\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 3 has a total capacity of 23.65 GiB of which 3.19 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "distance_matrix=distance_matrix_chunked(torch.from_numpy(array_all), weights, len(tensors)//512)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = np.zeros((array_size,array_size))\n",
    "\n",
    "for i in tqdm(range(array_size)):\n",
    "    for j in range(array_size):\n",
    "        dist = weighted\n",
    "        distance_matrix[i][j] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/129 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (234,1,8) (63,1,8) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdistance_matrix_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marray_all\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mdistance_matrix_chunked\u001b[0;34m(tensors, weights, chunk_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m tensor_chunk_j \u001b[38;5;241m=\u001b[39m tensors[j:end_j, np\u001b[38;5;241m.\u001b[39mnewaxis]  \u001b[38;5;66;03m# [1, chunk_size, dim]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 差の計算\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43mtensor_chunk_i\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_chunk_j\u001b[49m)  \u001b[38;5;66;03m# [chunk_size, chunk_size, dim]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m aw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog1p(np\u001b[38;5;241m.\u001b[39mexp(weights))  \u001b[38;5;66;03m# weightsのSoftplus関数を近似\u001b[39;00m\n\u001b[1;32m     24\u001b[0m aw \u001b[38;5;241m=\u001b[39m aw[np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis, :]  \u001b[38;5;66;03m# [1, 1, dim]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (234,1,8) (63,1,8) "
     ]
    }
   ],
   "source": [
    "distance_matrix_chunked(array_all, weights_np, len(array_all)//128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_matrix(data, distance_func):\n",
    "    num_points = data.shape[0]\n",
    "    cost_matrix = np.zeros((num_points, num_points))\n",
    "\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            if i != j:\n",
    "                cost_matrix[i, j] = distance_func(data[i], data[j])\n",
    "    \n",
    "    return cost_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix = compute_cost_matrix(data_a, custom_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein距離の計測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-EWT, chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoNLLU_EWT_PATH = \"corpora/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_EWT_PATH)\n",
    "EWT_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_GPT_PATH = \"corpora/English-chatGPT.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_GPT_PATH)\n",
    "GPT_tree_count = len(CoNLLU) - EWT_tree_count\n",
    "\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_EWT = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:EWT_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_GPT = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[EWT_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_en_corpora_EWT_Atis_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_EWT[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for _ in range(EWT_tree_count):\n",
    "    a.append(1/EWT_tree_count)\n",
    "b = []\n",
    "for _ in range(GPT_tree_count):\n",
    "    b.append(1/GPT_tree_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2fb53123d74474a06c01f1462cf3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_matrix = torch.zeros((EWT_tree_count, GPT_tree_count))\n",
    "\n",
    "tensors_EWT = torch.stack([t.to(\"cuda\") for t in tensors_EWT])\n",
    "tensors_GPT = torch.stack([t.to(\"cuda\") for t in tensors_GPT])\n",
    "\n",
    "for i in tqdm(range(EWT_tree_count)):\n",
    "    t_ewt = tensors_EWT[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_GPT, t_ewt.repeat(tensors_GPT.size(0), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5900653316080489"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EWT - EWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163bc9a5ebba45c996c6df7a928af39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94987be1c94453797f31e339f4425e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39c6bf284e348f89236ba45f2379bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[cost matrix]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/English/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_source_PATH)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/English/English-EWT.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_target_PATH)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_en_corpora_EWT_EWT_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda\") for t in tensors_target])\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1))\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EWT-ESL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16621 5124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f043bd74c6974b7ca9f2ba2aa00a90d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cb4b4453594d0cbb092e9514ebdd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/5124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb39f1b18b6546bc901bffa4e169db42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[cost matrix]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.99661802692641\n"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/English/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_source_PATH)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/English/English-ESL.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_target_PATH)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "print(source_tree_count, target_tree_count)\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_en_corpora_En_EWT_ESL_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda:3\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda:3\") for t in tensors_target])\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1))\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EWT - Atis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16621 5432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c857dd22e5e140e4ab00ccd9fa908e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cfe25cb7ef4b2390aa718d04985d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/5432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0081260c081496ab575dc12197e8802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[cost matrix]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.267264440750935\n"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/English/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_source_PATH)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/English/English-Atis.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_target_PATH)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "print(source_tree_count, target_tree_count)\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_en_corpora_EWT_Atis_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda:3\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda:3\") for t in tensors_target])\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1))\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EWT - Ja-BCCWJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16621 16621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c769e30a75e14ff39a46df36c21f467f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8df1dd2507f425ca95596d7bbff7123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc7f683a149408e8358e5265bf28b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[cost matrix]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9210620399488905\n"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/English/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_source_PATH)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/Japanese/Japanese-BCCWJ.conllu\"\n",
    "CoNLLU += random.sample(pyconll.load_from_file(CoNLLU_target_PATH), k=30000)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "print(source_tree_count, target_tree_count)\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_en_corpora_EWT_Ja-BCCWJ_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda:3\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda:3\") for t in tensors_target])\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1))\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En-EWT -- Fr-GSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16621 14450\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ddf720c3ff438ba036db9bb1f10b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a53f2947fae4a76a0e5238951edd3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/14450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6aaf70d8d245a5922747562864f842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[cost matrix]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.909186519127199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamazoe/.pyenv/versions/3.11.7/envs/env_pq/lib/python3.11/site-packages/ot/lp/__init__.py:580: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  check_result(result_code)\n"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/English/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_source_PATH)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/French/French-GSD.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_target_PATH)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "print(source_tree_count, target_tree_count)\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_En-EWT_Fr-GSD_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda:3\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda:3\") for t in tensors_target])\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1))\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EWT -- Korean-Kaist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16621 23010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4be19767d2a4e629f08701792af850b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33da7cb5580d416bbbfc3fdf8c5bbe14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/23010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19237a6a9c0d4a3f9ed1dc67eaf163e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[cost matrix]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.889577513517188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamazoe/.pyenv/versions/3.11.7/envs/env_pq/lib/python3.11/site-packages/ot/lp/__init__.py:580: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  check_result(result_code)\n"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/English/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_source_PATH)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/Korean/Korean-Kaist.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_target_PATH)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "print(source_tree_count, target_tree_count)\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_En-EWT_Ko-Kaist_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda:3\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda:3\") for t in tensors_target])\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1))\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fr-GSD -- Ja-BCCWJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 14450\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62227c3f7244e3197a1602fd208b38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaeabae0df6d4cbaa44a00a41e664158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/14450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7486852b08254dddb992ef5e045d1dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[cost matrix]:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4892249959879988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamazoe/.pyenv/versions/3.11.7/envs/env_pq/lib/python3.11/site-packages/ot/lp/__init__.py:580: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  check_result(result_code)\n"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/Japanese/Japanese-BCCWJ.conllu\"\n",
    "CoNLLU = random.sample(pyconll.load_from_file(CoNLLU_source_PATH), k=30000)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/French/French-GSD.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_target_PATH)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "print(source_tree_count, target_tree_count)\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_Fr-GSD_Ja-BCCWJ_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda:3\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda:3\") for t in tensors_target])\n",
    "\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1)).to(\"cpu\")\n",
    "\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fr-GSD -- Korean-kaist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23010 14450\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8dfdcf2c3742a3a4dbe5dbec5196ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/23010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b2e656970f4013ace9e028cc42ce59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/14450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c83dbbb4ac4f9a9bfdf13b3e2693b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[cost matrix]:   0%|          | 0/23010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.481494471447322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamazoe/.pyenv/versions/3.11.7/envs/env_pq/lib/python3.11/site-packages/ot/lp/__init__.py:580: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  check_result(result_code)\n"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/Korean/Korean-Kaist.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_source_PATH)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/French/French-GSD.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_target_PATH)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "print(source_tree_count, target_tree_count)\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_Fr-GSD_Ko-Kaist_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda:3\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda:3\") for t in tensors_target])\n",
    "\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1)).to(\"cpu\")\n",
    "\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ja-BCCWJ -- Ko-Kaist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23010 30000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b452034540840ec8f21789d55cb0027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/23010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aeaffb0e144409d93dcbf16fd4a26c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/model_Ko-Kaist_Ja-BCCWJ_unlabel_50.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/model_Ko-Kaist_Ja-BCCWJ_unlabel_50.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m distance_function \u001b[38;5;241m=\u001b[39m w_pq\u001b[38;5;241m.\u001b[39mWeightedPqgramDistance(tensors_source[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(), [], [])\n\u001b[0;32m---> 25\u001b[0m distance_function\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m distance_function\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m distance_function\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/env_pq/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/env_pq/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/env_pq/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/model_Ko-Kaist_Ja-BCCWJ_unlabel_50.pth'"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/Korean/Korean-Kaist.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_source_PATH)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/Japanese/Japanese-BCCWJ.conllu\"\n",
    "CoNLLU += random.sample(pyconll.load_from_file(CoNLLU_target_PATH), k=30000)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "print(source_tree_count, target_tree_count)\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_Ko-Kaist_Ja-BCCWJ_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda:3\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda:3\") for t in tensors_target])\n",
    "\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1)).to(\"cpu\")\n",
    "\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
