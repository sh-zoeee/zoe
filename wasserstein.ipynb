{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import ot\n",
    "\n",
    "from scripts import w_pq_batch as w_pq\n",
    "from scripts import trees, pq_gram, func\n",
    "\n",
    "from pqgrams.PQGram import Profile\n",
    "import pyconll\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EWT,Atis間のWasserstein距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors_path = \"data/train_tensors_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "train_labels_path = \"data/train_labels_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "train_indexes_path = \"data/train_indexes_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "\n",
    "valid_tensors_path = \"data/valid_tensors_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "valid_labels_path = \"data/valid_labels_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "valid_indexes_path = \"data/valid_indexes_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "\n",
    "test_tensors_path = \"data/test_tensors_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "test_labels_path = \"data/test_labels_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "test_indexes_path = \"data/test_indexes_en_corpora_En_EWT_Atis_unlabel_50.pt\"\n",
    "\n",
    "model_path = \"models/model_en_corpora_EWT_Atis_unlabel_50.pth\"\n",
    "\n",
    "CoNLLU_EWT_PATH = \"corpora/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_EWT_PATH)\n",
    "EWT_tree_count = len(CoNLLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = torch.load(train_tensors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-2.0425, -2.2598, -1.7006, -2.2871, -2.0425,  0.3741, -1.7006, -1.7006],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_function = w_pq.WeightedPqgramDistance(train_tensors[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTマトリックス: 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "a = [0.3, 0.7]  # ソース分布（サイズ2）\n",
    "b = [0.4, 0.4, 0.2]  # ターゲット分布（サイズ3）\n",
    "M = [[0.0, 1.0, 2.0],  # 2x3 のコスト行列\n",
    "     [1.0, 0.5, 0.5]]\n",
    "\n",
    "T = ot.emd2(a, b, M)\n",
    "print(\"OTマトリックス:\", T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoNLLU_GPT_PATH = \"corpora/English-chatGPT.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[convert tensor]: 100%|██████████| 30015/30015 [00:00<00:00, 41490.56it/s]\n"
     ]
    }
   ],
   "source": [
    "CoNLLU += pyconll.load_from_file(CoNLLU_GPT_PATH)\n",
    "\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index, desc=\"[convert tensor]\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_distance_weighted(data1, data2, weights):\n",
    "    # 重みを適用\n",
    "    weights = func.softplus(weights)\n",
    "    weighted_data1 = [t*weights for t in data1]  # 各サンプルに重みを適用\n",
    "    weighted_data2 = [t*weights for t in data2]\n",
    "\n",
    "    distances = []\n",
    "    for dim in range(weighted_data1[0].size(0)):  # 8次元でループ\n",
    "        # dim次元の要素を全て取得して連結\n",
    "        x_dim = torch.cat([t[dim].unsqueeze(0) for t in weighted_data1])\n",
    "        y_dim = torch.cat([t[dim].unsqueeze(0) for t in weighted_data2])\n",
    "\n",
    "        # 次元ごとに要素をソート\n",
    "        x_sorted = torch.sort(x_dim)[0]\n",
    "        y_sorted = torch.sort(y_dim)[0]\n",
    "\n",
    "        # 累積分布関数 (CDF) を計算\n",
    "        cdf_x = torch.cumsum(torch.ones_like(x_sorted) / len(x_sorted), dim=0)\n",
    "        cdf_y = torch.cumsum(torch.ones_like(y_sorted) / len(y_sorted), dim=0)\n",
    "\n",
    "        # 各次元のWasserstein距離を計算\n",
    "        distance = torch.mean(torch.abs(cdf_x - cdf_y))\n",
    "        distances.append(distance)\n",
    "\n",
    "    print(distances)\n",
    "    # 各次元の距離の平均を返す\n",
    "    return torch.mean(torch.tensor(distances)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightedPqgramDistance()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_function = w_pq.WeightedPqgramDistance(tensors[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = distance_function.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13394\n"
     ]
    }
   ],
   "source": [
    "print(min(EWT_tree_count, len(tensors)-EWT_tree_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_EWT = tensors[:EWT_tree_count]\n",
    "tensors_GPT = tensors[EWT_tree_count:]\n",
    "sample_size = min(EWT_tree_count, len(tensors)-EWT_tree_count)\n",
    "if EWT_tree_count<sample_size:\n",
    "    tensors_GPT = random.sample(tensors_GPT, k=sample_size)\n",
    "else :\n",
    "    tensors_EWT = random.sample(tensors_EWT, k=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensors_EWT = torch.zeros((EWT_tree_count,8))\n",
    "for i, tensor in enumerate(tensors_EWT):\n",
    "    Tensors_EWT[i] = tensor\n",
    "\n",
    "Tensors_GPT = torch.zeros((len(tensors_GPT),8))\n",
    "for i, tensor in enumerate(tensors_GPT):\n",
    "    Tensors_GPT[i] = tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix_chunked(tensors: np.ndarray, weights: np.ndarray, chunk_size: int):\n",
    "    \"\"\"\n",
    "    データをチャンクに分割して距離行列を計算する関数。\n",
    "    tensors: 入力データ [N, dim] の配列\n",
    "    weights: 重み [dim] の配列\n",
    "    chunk_size: 一度に処理するデータのチャンクサイズ\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples = tensors.shape[0]\n",
    "    dist_mat = np.zeros((num_samples, num_samples))  # 距離行列の初期化\n",
    "    \n",
    "    # チャンクごとに計算\n",
    "    for i in tqdm(range(0, num_samples, chunk_size)):\n",
    "        end_i = min(i + chunk_size, num_samples)\n",
    "        tensor_chunk_i = tensors[i:end_i, np.newaxis]  # [chunk_size, 1, dim]\n",
    "\n",
    "        for j in range(0, num_samples, chunk_size):\n",
    "            end_j = min(j + chunk_size, num_samples)\n",
    "            tensor_chunk_j = tensors[j:end_j, np.newaxis]  # [1, chunk_size, dim]\n",
    "            \n",
    "            # 差の計算\n",
    "            diff = np.abs(tensor_chunk_i - tensor_chunk_j)  # [chunk_size, chunk_size, dim]\n",
    "            aw = np.log1p(np.exp(weights))  # weightsのSoftplus関数を近似\n",
    "            aw = aw[np.newaxis, np.newaxis, :]  # [1, 1, dim]\n",
    "            weighted_diff = diff * aw  # アダマール積\n",
    "            dist_chunk = weighted_diff.sum(axis=2)  # 距離の計算\n",
    "            \n",
    "            # 距離行列に結果を格納\n",
    "            dist_mat[i:end_i, j:end_j] = dist_chunk\n",
    "\n",
    "    return dist_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16621\n"
     ]
    }
   ],
   "source": [
    "array_EWT = Tensors_EWT.numpy()\n",
    "array_GPT = Tensors_GPT.numpy()\n",
    "\n",
    "weights_np = weights.detach().numpy()\n",
    "\n",
    "array_size = len(array_EWT) + len(array_GPT)\n",
    "\n",
    "array_all = np.zeros((array_size, 8))\n",
    "\n",
    "print(len(array_EWT))\n",
    "\n",
    "for i in range(len(array_EWT)):\n",
    "    array_all[i] = array_EWT[i]\n",
    "\n",
    "for i in range(len(array_GPT)):\n",
    "    array_all[i+len(array_GPT)] = array_GPT[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix_chunked(tensors: torch.Tensor, weights: torch.Tensor, chunk_size: int):\n",
    "    \"\"\"\n",
    "    データをチャンクに分割して距離行列を計算する関数。\n",
    "    tensors: 入力データ [N, dim] のテンソル\n",
    "    weights: 重み [dim] のテンソル\n",
    "    chunk_size: 一度に処理するデータのチャンクサイズ\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    num_samples = tensors.shape[0]\n",
    "    dist_mat = torch.zeros((num_samples, num_samples), device=device)  # 距離行列の初期化\n",
    "    \n",
    "    # チャンクごとに計算\n",
    "    for i in range(0, num_samples, chunk_size):\n",
    "        end_i = min(i + chunk_size, num_samples)\n",
    "        tensor_chunk_i = tensors[i:end_i].unsqueeze(1)  # [chunk_size, 1, dim]\n",
    "\n",
    "        for j in range(0, num_samples, chunk_size):\n",
    "            end_j = min(j + chunk_size, num_samples)\n",
    "            tensor_chunk_j = tensors[j:end_j].unsqueeze(0)  # [1, chunk_size, dim]\n",
    "            \n",
    "            # 差の計算\n",
    "            diff = torch.abs(tensor_chunk_i - tensor_chunk_j).to(device)  # [chunk_size, chunk_size, dim]\n",
    "            aw = func.softplus(weights).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, dim]\n",
    "            weighted_diff = diff * aw  # アダマール積\n",
    "            dist_chunk = weighted_diff.sum(dim=2)  # 距離の計算\n",
    "            \n",
    "            # 距離行列に結果を格納\n",
    "            dist_mat[i:end_i, j:end_j] = dist_chunk\n",
    "        del dist_chunk, end_j, aw, weighted_diff\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    del tensors, end_i\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 3 has a total capacity of 23.65 GiB of which 3.19 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m distance_matrix\u001b[38;5;241m=\u001b[39m\u001b[43mdistance_matrix_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_all\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mdistance_matrix_chunked\u001b[0;34m(tensors, weights, chunk_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m diff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(tensor_chunk_i \u001b[38;5;241m-\u001b[39m tensor_chunk_j)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# [chunk_size, chunk_size, dim]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m aw \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39msoftplus(weights)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [1, 1, dim]\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m weighted_diff \u001b[38;5;241m=\u001b[39m \u001b[43mdiff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maw\u001b[49m  \u001b[38;5;66;03m# アダマール積\u001b[39;00m\n\u001b[1;32m     26\u001b[0m dist_chunk \u001b[38;5;241m=\u001b[39m weighted_diff\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 距離の計算\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 距離行列に結果を格納\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 3 has a total capacity of 23.65 GiB of which 3.19 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "distance_matrix=distance_matrix_chunked(torch.from_numpy(array_all), weights, len(tensors)//512)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = np.zeros((array_size,array_size))\n",
    "\n",
    "for i in tqdm(range(array_size)):\n",
    "    for j in range(array_size):\n",
    "        dist = weighted\n",
    "        distance_matrix[i][j] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/129 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (234,1,8) (63,1,8) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdistance_matrix_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marray_all\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mdistance_matrix_chunked\u001b[0;34m(tensors, weights, chunk_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m tensor_chunk_j \u001b[38;5;241m=\u001b[39m tensors[j:end_j, np\u001b[38;5;241m.\u001b[39mnewaxis]  \u001b[38;5;66;03m# [1, chunk_size, dim]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 差の計算\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43mtensor_chunk_i\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_chunk_j\u001b[49m)  \u001b[38;5;66;03m# [chunk_size, chunk_size, dim]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m aw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog1p(np\u001b[38;5;241m.\u001b[39mexp(weights))  \u001b[38;5;66;03m# weightsのSoftplus関数を近似\u001b[39;00m\n\u001b[1;32m     24\u001b[0m aw \u001b[38;5;241m=\u001b[39m aw[np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis, :]  \u001b[38;5;66;03m# [1, 1, dim]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (234,1,8) (63,1,8) "
     ]
    }
   ],
   "source": [
    "distance_matrix_chunked(array_all, weights_np, len(array_all)//128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_matrix(data, distance_func):\n",
    "    num_points = data.shape[0]\n",
    "    cost_matrix = np.zeros((num_points, num_points))\n",
    "\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            if i != j:\n",
    "                cost_matrix[i, j] = distance_func(data[i], data[j])\n",
    "    \n",
    "    return cost_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix = compute_cost_matrix(data_a, custom_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein距離の計測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-EWT, chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c739c5c6ba343e989b69cd03849323e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b714133c4f4462968dd4132898dc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/13394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CoNLLU_EWT_PATH = \"corpora/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_EWT_PATH)\n",
    "EWT_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_GPT_PATH = \"corpora/English-chatGPT.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_GPT_PATH)\n",
    "GPT_tree_count = len(CoNLLU) - EWT_tree_count\n",
    "\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_EWT = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:EWT_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_GPT = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[EWT_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_en_corpora_EWT_Atis_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_EWT[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for _ in range(EWT_tree_count):\n",
    "    a.append(1/EWT_tree_count)\n",
    "b = []\n",
    "for _ in range(GPT_tree_count):\n",
    "    b.append(1/GPT_tree_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2fb53123d74474a06c01f1462cf3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_matrix = torch.zeros((EWT_tree_count, GPT_tree_count))\n",
    "\n",
    "tensors_EWT = torch.stack([t.to(\"cuda\") for t in tensors_EWT])\n",
    "tensors_GPT = torch.stack([t.to(\"cuda\") for t in tensors_GPT])\n",
    "\n",
    "for i in tqdm(range(EWT_tree_count)):\n",
    "    t_ewt = tensors_EWT[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_GPT, t_ewt.repeat(tensors_GPT.size(0), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5900653316080489"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceff0fb65b554b7bb78b4aa7cf864bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9567115b66b4469a1fa74e2580ac48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[convert tensor]:   0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8a844529004dada2f452d49afaa441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "CoNLLU_source_PATH = \"corpora/English/English-EWT.conllu\"\n",
    "CoNLLU = pyconll.load_from_file(CoNLLU_source_PATH)\n",
    "source_tree_count = len(CoNLLU)\n",
    "\n",
    "CoNLLU_target_PATH = \"corpora/English/English-EWT.conllu\"\n",
    "CoNLLU += pyconll.load_from_file(CoNLLU_target_PATH)\n",
    "target_tree_count = len(CoNLLU) - source_tree_count\n",
    "\n",
    "\n",
    "PQ_Trees = [trees.conllTree_to_pqTree_unlabeled(conll.to_tree()) for conll in CoNLLU]\n",
    "PQ_Index = [Profile(tree, p=2, q=2) for tree in PQ_Trees]\n",
    "\n",
    "J = set(PQ_Index[0])\n",
    "for pq_set  in PQ_Index[1:]:\n",
    "    J = J.union(pq_set)\n",
    "J = list(J)\n",
    "\n",
    "tensors_source = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[:source_tree_count], desc=\"[convert tensor]\")]\n",
    "tensors_target = [pq_gram.pqgram_to_tensor(pqgram, J) for pqgram in tqdm(PQ_Index[source_tree_count:], desc=\"[convert tensor]\")]\n",
    "\n",
    "\n",
    "model_path = \"models/model_en_corpora_EWT_EWT_unlabel_50.pth\"\n",
    "distance_function = w_pq.WeightedPqgramDistance(tensors_source[0].size(), [], [])\n",
    "distance_function.load_state_dict(torch.load(model_path))\n",
    "distance_function.eval()\n",
    "weights = distance_function.weights\n",
    "\n",
    "a = []\n",
    "for _ in range(source_tree_count):\n",
    "    a.append(1/source_tree_count)\n",
    "b = []\n",
    "for _ in range(target_tree_count):\n",
    "    b.append(1/target_tree_count)\n",
    "\n",
    "cost_matrix = torch.zeros((source_tree_count, target_tree_count))\n",
    "\n",
    "tensors_source = torch.stack([t.to(\"cuda\") for t in tensors_source])\n",
    "tensors_target = torch.stack([t.to(\"cuda\") for t in tensors_target])\n",
    "\n",
    "for i in tqdm(range(source_tree_count), desc=\"[cost matrix]\"):\n",
    "    t_source = tensors_source[i].unsqueeze(0)\n",
    "    cost_matrix[i] = w_pq.weighted_pqgram_distance_batch(weights, tensors_target, t_source.repeat(tensors_target.size(0), 1))\n",
    "\n",
    "print(ot.emd2(a, b, cost_matrix.detach().numpy(), numItermax=1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
